{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf2Y4ptvAnBx",
        "outputId": "8d039933-cece-416a-b122-fafed1b1cd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets timm lightning --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlKMGFu8BEcm",
        "outputId": "f3647f4d-7d92-4cfe-ac74-a67296e1570c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading chest-xray-pneumonia.zip to ./chest-xray-pneumonia\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.29G/2.29G [01:46<00:00, 23.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "# Assign the Kaggle data set URL into variable\n",
        "dataset = 'https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data'\n",
        "od.download(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml0Y6xNpBF5v",
        "outputId": "ffbabbe5-6c07-4190-e878-e58be155e1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# custom modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import utilities\n",
        "#import model_functions\n",
        "import model_factory\n",
        "\n",
        "#lightning modules and callbacks\n",
        "import lightning_data\n",
        "import lightning_model\n",
        "import train_info\n",
        "import learning_curves\n",
        "import confusion_matrix\n",
        "\n",
        "import os\n",
        "\n",
        "# timm models\n",
        "import timm\n",
        "\n",
        "# torch modules (temporarily)\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# pytorch lightning (for checkpointing callbacks)\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcPhYsM5BJse",
        "outputId": "90aa689c-dbd0-49fd-d225-5117256a0050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# necessary, as checkpoints will be saved on GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEXXMbDS0E4G",
        "outputId": "d3f6c879-e6a2-4e31-e01b-0a10f787b109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23,512,130 total parameters.\n",
            "23,512,130 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "# ResNet50\n",
        "'''\n",
        "arch_name = 'resnet50'\n",
        "classifier_name = 'linear'\n",
        "classifier_type = model_factory.get_linear_classifer if classifier_name == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "layers_version = 'all'\n",
        "data_aug_name = \"strong\"\n",
        "layers_version_name = layers_version + data_aug_name\n",
        "\n",
        "model1, config1 = model_factory.get_model(arch_name, classifier_type, layers_version)\n",
        "\n",
        "check_point_path_1 = '/content/drive/MyDrive/models/resnet50-best/epoch=4-step=370.ckpt'\n",
        "'''\n",
        "\n",
        "resnet50_config = {\n",
        "    'model_name': 'resnet50', # name of the pretrained model\n",
        "    'classifier_name': 'linear', # name of the classifier (e.g. linear/nonlinear)\n",
        "    'classifier_type': None, # leave it None\n",
        "    'layers': 'all', # layers to train (e.g. first (starting from last), second (starting from last), all)\n",
        "    'augmentation': 'strong', # augmentation type (e.g. normal or strong)\n",
        "    'classes_weight': None, # weights for each class\n",
        "    'batch_size': 64,\n",
        "    'val_split': 0.1,\n",
        "    'n_epochs': 20,\n",
        "    'optimizer': 'Adam',\n",
        "    'scheduler': '', # leave it empty to not use any scheduling\n",
        "    'ensemble': True,\n",
        "    'image_size': None,\n",
        "    'mean': None,\n",
        "    'std': None\n",
        "    }\n",
        "\n",
        "resnet50_config['classifier_type'] = model_factory.get_linear_classifer if config['classifier_name'] == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "\n",
        "resnet50_ckpt = '/content/drive/MyDrive/models/resnet50/.../epoch={}-step={}.ckpt'\n",
        "l_resnet50_model = lightning_model.PneumoniaModel.load_from_checkpoint(resnet50_ckpt, h=resnet50_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1niiOyi-0hPI",
        "outputId": "7a130013-16ae-4fac-aa4b-d67883717141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6,955,906 total parameters.\n",
            "6,955,906 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "# DenseNet121\n",
        "'''\n",
        "arch_name = 'densenet121'\n",
        "classifier_name = 'linear'\n",
        "classifier_type = model_factory.get_linear_classifer if classifier_name == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "layers_version = 'all'\n",
        "data_aug_name = \"strong\"\n",
        "\n",
        "model2, config2 = model_factory.get_model(arch_name, classifier_type, layers_version)\n",
        "\n",
        "check_point_path_2 = '/content/drive/MyDrive/models/densenet121-best/epoch=6-step=518.ckpt'\n",
        "'''\n",
        "\n",
        "densenet121_config = {\n",
        "    'model_name': 'densenet121', # name of the pretrained model\n",
        "    'classifier_name': 'linear', # name of the classifier (e.g. linear/nonlinear)\n",
        "    'classifier_type': None, # leave it None\n",
        "    'layers': 'all', # layers to train (e.g. first (starting from last), second (starting from last), all)\n",
        "    'augmentation': 'strong', # augmentation type (e.g. normal or strong)\n",
        "    'classes_weight': None, # weights for each class\n",
        "    'batch_size': 64,\n",
        "    'val_split': 0.1,\n",
        "    'n_epochs': 20,\n",
        "    'optimizer': 'SGD',\n",
        "    'scheduler': 'CosineAnnealingLR10', # leave it empty to not use any scheduling\n",
        "    'ensemble': True,\n",
        "    'image_size': None,\n",
        "    'mean': None,\n",
        "    'std': None\n",
        "    }\n",
        "\n",
        "densenet121_config['classifier_type'] = model_factory.get_linear_classifer if config['classifier_name'] == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "\n",
        "densenet121_ckpt = '/content/drive/MyDrive/models/densenet121/.../epoch={}-step={}.ckpt'\n",
        "l_densenet121_model = lightning_model.PneumoniaModel.load_from_checkpoint(densenet121_ckpt, h=densenet121_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbEvqskW0j4X",
        "outputId": "9853c193-e78b-4bbb-b1f3-e61a263b7f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4,010,110 total parameters.\n",
            "4,010,110 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "# EfficientNet_b0\n",
        "'''\n",
        "arch_name = 'efficientnet_b0'\n",
        "classifier_name = 'linear'\n",
        "classifier_type = model_factory.get_linear_classifer if classifier_name == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "layers_version = 'all'\n",
        "data_aug_name = \"strong\"\n",
        "\n",
        "model3, config3 = model_factory.get_model(arch_name, classifier_type, layers_version)\n",
        "\n",
        "check_point_path_3 = '/content/drive/MyDrive/models/efficientnet-best/epoch=8-step=666.ckpt'\n",
        "'''\n",
        "\n",
        "efficientnet_b0_config = {\n",
        "    'model_name': 'efficientnet_b0', # name of the pretrained model\n",
        "    'classifier_name': 'linear', # name of the classifier (e.g. linear/nonlinear)\n",
        "    'classifier_type': None, # leave it None\n",
        "    'layers': 'all', # layers to train (e.g. first (starting from last), second (starting from last), all)\n",
        "    'augmentation': 'strong', # augmentation type (e.g. normal or strong)\n",
        "    'classes_weight': None, # weights for each class\n",
        "    'batch_size': 64,\n",
        "    'val_split': 0.1,\n",
        "    'n_epochs': 20,\n",
        "    'optimizer': 'SGD',\n",
        "    'scheduler': '', # leave it empty to not use any scheduling\n",
        "    'ensemble': True,\n",
        "    'image_size': None,\n",
        "    'mean': None,\n",
        "    'std': None\n",
        "    }\n",
        "\n",
        "efficientnet_b0['classifier_type'] = model_factory.get_linear_classifer if config['classifier_name'] == 'linear' else model_factory.get_simple_non_linear_classifier\n",
        "\n",
        "efficientnet_b0_ckpt = '/content/drive/MyDrive/models/efficientnet_b0/.../epoch={}-step={}.ckpt'\n",
        "l_efficientnet_b0_model = lightning_model.PneumoniaModel.load_from_checkpoint(efficientnet_b0_ckpt, h=efficientnet_b0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVQPJdQqBLj9"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "'''\n",
        "l_model1 = lightning_model.PneumoniaModel.load_from_checkpoint(check_point_path_1, h=config1)\n",
        "l_model2 = lightning_model.PneumoniaModel.load_from_checkpoint(check_point_path_2, h=config2)\n",
        "l_model3 = lightning_model.PneumoniaModel.load_from_checkpoint(check_point_path_3, h=config3)\n",
        "\n",
        "# print(l_model1.test_acc * 100)\n",
        "# print(l_model2.test_acc * 100)\n",
        "# print(l_model3.test_acc * 100)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6RTjGWB61Gu",
        "outputId": "35812e75-d96d-4ba1-afd2-867bb5c47b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (drop_block): Identity()\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (aa): Identity()\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=2, bias=True)\n",
            "    (1): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "model1 = l_model1.model\n",
        "model2 = l_model2.model\n",
        "model3 = l_model3.model\n",
        "\n",
        "print(model1)\n",
        "'''\n",
        "\n",
        "resnet50_model = l_resnet50_model.model\n",
        "densenet121_model = l_densenet121.model\n",
        "efficientnet_b0_model = l_efficientnet_b0_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUxMAomNBbsQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "model1.fc = nn.Sequential(*list(model1.fc.children())[:-1])\n",
        "model2.classifier = nn.Sequential(*list(model2.classifier.children())[:-1])\n",
        "model3.classifier = nn.Sequential(*list(model3.classifier.children())[:-1])\n",
        "'''\n",
        "\n",
        "resnet50_model.fc = nn.Sequential(*list(resnet50_model.fc.children())[:-1])\n",
        "densenet121_model.classifier = nn.Sequential(*list(densenet121_model.classifier.children())[:-1])\n",
        "efficientnet_b0_model.classifier = nn.Sequential(*list(efficientnet_b0_model.classifier.children())[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4xYB-cICKVw"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwTc7LDB4Gka"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "config = {\n",
        "    'model_name': 'efficientnet_b0', # name of the pretrained model\n",
        "    'classifier_name': 'linear', # name of the classifier (e.g. linear/nonlinear)\n",
        "    'classifier_type': None, # leave it None\n",
        "    'layers': 'classifier', # layers to train (e.g. first (starting from last), second (starting from last), all)\n",
        "    'augmentation': 'normal', # augmentation type (e.g. normal or strong)\n",
        "    'classes_weight': None, # weights for each class\n",
        "    'batch_size': 128,\n",
        "    'val_split': 0.1,\n",
        "    'n_epochs': 20,\n",
        "    'optimizer': 'Adam',\n",
        "    'scheduler': '', # leave it empty to not use any scheduling\n",
        "    'image_size': None,\n",
        "    'mean': None,\n",
        "    'std': None\n",
        "    }\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7iDp2-_Cq4y"
      },
      "outputs": [],
      "source": [
        "# dataloader\n",
        "pneumonia_data_resnet50 = lightning_data.PneumoniaDataModule(resnet50_config)\n",
        "pneumonia_data_resnet50.setup()\n",
        "test_set = pneumonia_data_resnet50.test_set\n",
        "print(len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_DnIGb2CtAC",
        "outputId": "0f8755fe-9fe5-4bcd-8dc9-c3449a06507e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "624\n"
          ]
        }
      ],
      "source": [
        "# Can I do this?\n",
        "'''\n",
        "pneumonia_data.setup()\n",
        "test_set = pneumonia_data.test_set\n",
        "print(len(test_set))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf2Yhh0bDGNZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def create_meta_data(test_loader, model_list):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    meta_X = []\n",
        "    meta_Y = []\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.unsqueeze(0)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        logits_list = []\n",
        "\n",
        "        for model in model_list:\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(inputs)\n",
        "\n",
        "            logits_list.append(logits.cpu().numpy())\n",
        "\n",
        "        meta_X.append(np.concatenate(logits_list, axis=1))\n",
        "        meta_Y.append(labels)\n",
        "\n",
        "    meta_X = np.concatenate(meta_X, axis=0)\n",
        "    meta_Y = np.array(meta_Y)\n",
        "\n",
        "    indices = np.arange(meta_X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    meta_X = meta_X[indices]\n",
        "    meta_Y = meta_Y[indices]\n",
        "\n",
        "    return meta_X, meta_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqxmVwcwEHEC"
      },
      "outputs": [],
      "source": [
        "#models = [model1, model2, model3]\n",
        "models = [resnet50_model, densenet121_model, efficientnet_b0_model]\n",
        "\n",
        "X, Y = create_meta_data(pneumonia_data.test_set, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwOOMuGGa7z",
        "outputId": "52dbbbb4-8bc7-4f8f-db09-e58928710b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(624, 6)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OH7xT9WG6q7",
        "outputId": "ca39e51a-ecf0-4877-86b9-4d4614cb16d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-4.312985    4.5012407  -3.2164967   2.694486   -1.7252353   2.0061772 ]\n",
            " [-5.0372353   5.251523   -4.092097    5.2101717  -2.74962     2.8244402 ]\n",
            " [-2.4904418   2.408436   -2.1623557   1.6424263  -1.1018068   0.9155461 ]\n",
            " ...\n",
            " [-1.8758922   1.858717    0.55103964 -0.46427637  0.10158892  0.13055673]\n",
            " [-3.8166263   3.8185513  -3.2125533   2.8137949  -2.1002035   2.4353764 ]\n",
            " [ 4.0108204  -4.0597777   4.0528765  -3.8711917   2.7060137  -2.5737817 ]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNzuh4fRG95I",
        "outputId": "30c8aa0d-a6f6-4dd6-b667-5123813bba11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(624,)\n"
          ]
        }
      ],
      "source": [
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_da5bec-sWGB",
        "outputId": "60e5d9b3-d4f9-4041-ed83-f029a84b6464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53mrDRROsXHs",
        "outputId": "0eaa5bb4-4e71-4296-9651-606231e52a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 0.944\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and Y are your features and target variable, respectively\n",
        "\n",
        "# Step 1: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8)\n",
        "\n",
        "# Step 2: Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Step 3: Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGvkdHfZ8vrL",
        "outputId": "8a27ac4f-ccbe-42fd-8ed5-ba0ec36333b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 0.944\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and Y are your features and target variable, respectively\n",
        "\n",
        "# Step 1: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8)\n",
        "\n",
        "# Step 2: Initialize the SVM model\n",
        "model = SVC()\n",
        "\n",
        "# Step 3: Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miwOuNXo9BqG",
        "outputId": "af48ef8c-d0ed-484d-de86-1d485a85a9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 0.916\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and Y are your features and target variable, respectively\n",
        "\n",
        "# Step 1: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Step 2: Initialize the MLP classifier with 2 hidden layers\n",
        "model = MLPClassifier(hidden_layer_sizes=(1000, 500), max_iter=1000)  # You can adjust the hidden layer sizes and other parameters\n",
        "\n",
        "# Step 3: Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
